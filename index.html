<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Drift Reduction in Underwater Egomotion Computation By Axial Camera Modeling">
  <meta name="keywords" content="Egomotion, Axial Camera, Refraction, Underwater imaging">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Axial camera egomotion</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!--
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
  -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/depicon.png">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script type="text/javascript" src="https://www.maths.nottingham.ac.uk/plp/pmadw/LaTeXMathML.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://www.researchgate.net/profile/Bashar-Elnashef">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <!-- <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a> -->
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Drift Reduction in Underwater Egomotion Computation By Axial Camera Modeling</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.researchgate.net/profile/Bashar-Elnashef">Bashar Elnashef</a><sup>*</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.researchgate.net/profile/Sagi-Filin">Sagi Filin</a><sup>*</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Technion - Israel Institute of Technology</span>
            <!--<span class="author-block"><sup>2</sup>Me</span>
            -->
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://www.researchgate.net/profile/Sagi-Filin"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- 
              <span class="link-block">
                <a href="https://www.researchgate.net/profile/Sagi-Filin"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.researchgate.net/profile/Sagi-Filin"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/bashar-elnashef/Axial-camera-underwater-egomotion-"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link.
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
                -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="hero teaser2">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/teaser.mp4"
                type="video/mp4">
      </video> -->
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="figures/maize.mp4"
                type="video/mp4"> -->
      <!--<img id="teaser" >
        <source src="figures/ceserea_flow.gif">
      </img> -->
      <img src="figures/ceserea_flow.gif" alt="description of gif" /> 
      <img src="figures/ceserea.gif" alt="description of gif" /> 
      <img src="figures/ceserea.gif" alt="description of gif" /> 
      <img src="figures/ceserea.gif" alt="description of gif" /> 
      <h2 class="subtitle">
        <span class="dnerf">Something something. 
      </h2>
    </div>
  </div>
</section>
<section class="hero teaser1">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/teaser.mp4"
                type="video/mp4">
      </video> -->
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="figures/maize.mp4"
                type="video/mp4"> -->
      <!--<img id="teaser" >
        <source src="figures/ceserea_flow.gif">
      </img> -->
      <img src="figures/ship_flow.gif" alt="description of gif" /> 
      <img src="figures/ship.gif" alt="description of gif" /> 
      <img src="figures/ship_flow.gif" alt="description of gif" /> 
      <img src="figures/ship.gif" alt="description of gif" /> 
      <h2 class="subtitle">
        <span class="dnerf">Something something. 
      </h2>
    </div>
  </div>
</section>
<section class="hero teaser3">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="https://homes.cs.washington.edu/~kpar/nerfies/videos/teaser.mp4"
                type="video/mp4">
      </video> -->
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="figures/maize.mp4"
                type="video/mp4"> -->
      <!--<img id="teaser" >
        <source src="figures/ceserea_flow.gif">
      </img> -->
      <img src="figures/A.gif" alt="description of gif" /> 
      <h2 class="subtitle">
        <span class="dnerf">Something something. 
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Localization of autonomous underwater vehicles is essential for underwater exploration, environmental monitoring,
            and infrastructure inspections, as only a few examples. As wirelessly transmitted data cannot be used in underwater navigation,
            and equivalent sensors are costly and not readily deployable, the use of cameras as a localization aid offers an attractive alternative.
            Nonetheless, the utilization of cameras for underwater navigation is hindered by medium-induced visual deterioration and refraction
            of the incident ray. While most research has been focusing on addressing the radiometric deterioration effects, we demonstrate
            that the introduction of a physically aware model of the refraction effect can significantly improve the platform localization. Specifically,
            we develop a refraction-aware continuous formulation of the egomotion model, which computes the translational and rotational
            velocities using optical flow measurements. We also demonstrate that a linear model can be reached despite the nonlinear rays
            path. Results show an improved navigation solution echoed by a significant reduction in the drift with more than tenfold improvement
            in the estimated position and angular quantities compared to state-of-the-art methods. They demonstrate the benefit of exact
            modeling in underwater navigation.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
    <!-- Paper Image. -->

    <!--/ Paper Image. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Pipeline overview</h2>
        <div class="content has-text-justified">
          Having established our egomotion computation form, our VO pipeline is the following: We initiate the solution by selecting two images with sufficient overlap and significant baseline, extracting and matching joint features, 
          and triangulating a set of 3-D points to their actual scale. We use them to propagate the scale through the trajectory. New frames are added, and features are tracked over frames using the KLT sparse optical flow algorithm. We estimate the relative pose between two frames using the proposed egomotion model and RanSaC. 
          Estimating the six degrees of freedom of the pose of every frame uses their respective 2-D-3-D correspondences. Having matched 2-D-3-D between 3-D triangulated points and 2-D features in the new frame, we estimate its pose by solving the perspective-n-point (PnP) problem. 
          This operation is carried out within a RanSaC loop to remove inaccurate correspondences. Next, new features are triangulated from the observed feature tracks into 3-D points using each track's first and last observation. The procedure is repeated as new frames are added.
          Notably, our pipeline here revolves around the egomotion computation, the focus of this paper, and does not introduce a BA computation for keyframes.
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <!-- <h2 class="title is-3">Pipeline overview</h2> -->
        <div class="publication-video">
          <img src="Figures/pipeline.png"
          class="interpolation-image"
          alt="Interpolate start reference image."/>
        </div>
      </div>
    </div>


</section>


<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="column is-four-fifths">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Axial modeling of egomotion estimation</h2>
        <div class="content has-text-justified">
          <p>
          The estimate the camera motion and scene depth by minimizing the nonlinear objective function,
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{elnashef2023drift,
  author    = {Elnashef, Bashar and Filin, Sagi},
  title     = {Drift Reduction in Underwater Egomotion Computation By Axial Camera Modeling},
  journal   = {},
  year      = {2023},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://homes.cs.washington.edu/~kpar/nerfies/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
